#!/usr/bin/env python3
# -*- coding: utf-8 -*-


import sys


# if not calling for snakemake rule
try:
    sys.stderr = open(snakemake.log[0], "w")
except NameError:
    pass


import json
import datetime
import pandas as pd
from enum import Enum
from typing import Optional, Annotated, Any, TypeVar, Union
from pydantic_core import PydanticCustomError
from pydantic import (
    BaseModel,
    ValidationError,
    PastDate,
    BeforeValidator,
    model_validator,
    field_validator,
)


# These fields must contain unique values:
UNIQUE_FIELDS = ['isolate_id', 'fasta_name', 'fasta_md5']


def coerce_nan_to_None(v: Any) -> Any:
    """
    Coercion of nan float values created by panda for empty fields to None
    where nescessary
    """
    if pd.isna(v):
        return None
    return v


NanOrNone = Annotated[Optional[TypeVar('T')], BeforeValidator(coerce_nan_to_None)]


class OrganismEnum(str, Enum):
    """
    Define accepted organisms
    """
    listeria = 'Listeria monocytogenes'
    salmonella = 'Salmonella enterica'
    ecoli = 'Escherichia coli'
    campy = 'Campylobacter spp.'


class UserEnum(str, Enum):
    """
    Define accepted users
    Ideally should come from Database, for future versions
    """
    owl = 'OWL'
    rrw = 'RRW'
    mel = 'MEL'
    wfl = 'WFL'
    rld = 'RLD'
    otther = 'other'


class SampleTypesEnum(str, Enum):
    """
    Define accepted sample types
    """
    food = "Lebensmittel"
    feed = "Futtermittel"
    env = "Umfeld"
    vet = "Tiergesundheit"
    human = "Human"
    other = "unknown"


class Metadata(BaseModel, validate_assignment=True):
    """
    Implements the GEÃœBt Metadata Model
    Version 2 (2024-03)
    """
    isolate_id: str
    sample_id: str
    alt_isolate_id: Optional[NanOrNone] = None
    organism: OrganismEnum
    isolation_org: UserEnum
    sequencing_org: UserEnum
    bioinformatics_org: UserEnum
    third_party_owner: Optional[NanOrNone] = None
    extraction_method: Optional[NanOrNone] = None
    library_kit: Optional[NanOrNone] = None
    sequencing_kit: Optional[NanOrNone] = None
    sequencing_instrument: Optional[NanOrNone] = None
    assembly_method: Optional[NanOrNone] = None
    sample_type: SampleTypesEnum
    fasta_name: str
    fasta_md5: str
    collection_date: Union[NanOrNone, PastDate] = datetime.date(1970, 1, 1)  # not working, coercing below
    customer: NanOrNone = "NNNNN"
    manufacturer: NanOrNone = "unknown"
    collection_place: NanOrNone = "Unbekannt (99999999)"
    collection_place_code: NanOrNone = "48267|177667|"
    description: NanOrNone = "unknown"
    manufacturer_type: Optional[NanOrNone] = None
    manufacturer_type_code: Optional[NanOrNone] = None
    matrix: Optional[NanOrNone] = None
    matrix_code: Optional[NanOrNone] = None
    collection_cause: Optional[NanOrNone] = None
    collection_cause_code: Optional[NanOrNone] = None
    lot_number: Optional[NanOrNone] = None
    seq_depth: float
    ref_coverage: float
    q30: float

    @field_validator('ref_coverage', 'q30')
    @classmethod
    def check_fractions(cls, v: float) -> float:
        if v < 0 or v > 1:
            raise ValueError("must be given as a fraction between 0 and 1")
        return v

    @field_validator('q30')
    @classmethod
    def check_q30(cls, v: float) -> float:
        if v < 0.75:
            raise ValueError("must be at least 0.75")
        return v

    @field_validator('collection_date', mode='after')
    @classmethod
    def coerce_date_to_iso(cls, d: datetime) -> str:
        if not d:
            d = "1970-01-01"
        try:
            d = datetime.date.fromisoformat(d)
        except ValueError:
            d = datetime.datetime.strptime(d, "%d.%m.%Y")
        return datetime.date.strftime(d, "%Y-%m-%d")

    @model_validator(mode='after')
    def check_coverage(self) -> 'Metadata':
        depth = self.seq_depth
        organism = self.organism
        min_coverages = {
            'Listeria monocytogenes': 20,
            'Salmonella enterica': 30,
            'Escherichia coli': 40,
            'Campylobacter spp.': 20,
        }
        if depth < min_coverages[organism] or depth > 200:
            raise PydanticCustomError(
                "value_error",
                f"Value error: 'coverage' for '{organism}' must be between "
                f"'{min_coverages[organism]}' and 200, got {depth}.",
            )
        return self


def validate_record(record: dict, model: BaseModel):
    """
    Validate a single record against a data model

    Each entry of the record is checked against the schema. If errors are
    encountered, all o them are formattred in a user-friendly way.
    The output is a tuple whose first element is the check result and the
    second element is the list of formatted error messages.

    Parameters
    ----------
    record : dict
        A dictionnary of key-values generated by json.load.
    validator: dict
        A valid instance of jsonschema.protocols.Validator.

    Returns
    -------
    tuple:
        a tuple (STATUS, Errors) where status is either of PASS or FAIL and
        Errors us a list of all validation errors encountered.
    """
    messages = []
    try:
        m = model.model_validate(record)
        status = "PASS"
    except ValidationError as e:
        status = "FAIL"
        m = {}
        for error in e.errors():
            # Make error pretty
            if len(error['loc']) == 1:
                msg = f"Invalid value in field '{error['loc'][0]}'. "
            elif len(error['loc']) > 1:
                msg = f"Invalid value in field '{error['loc']}'. "
            else:
                msg = ''
            if type(error['input']) is dict:
                msg += f"{error['msg']}"
            else:
                msg += f"'{error['msg']}', got '{error['input']}' instead"
            messages.append(msg)
    return status, messages, m


def main(metadata, json_path, tsv_path, metadata_json, data_model=Metadata):
    # load metadata as dataframe
    metatable = pd.read_csv(
        metadata,
        sep='\t',
        header=0,
        index_col=False
    )
    records = metatable.to_dict(orient='records')

    # Validating each entry indpendantly to salvage the good ones in case of fails
    valid_records = {}
    validation_status = {}
    for record in records:
        status, errors, parsed = validate_record(record, data_model)
        validation_status.update(
            {
                record['isolate_id']: {
                    "STATUS": status,
                    "MESSAGES": errors
                }
            }
        )
        # save parsed records
        if status == 'PASS':
            valid_records[record['isolate_id']] = parsed.model_dump()

    # export valid metadata to Json file
    with open(metadata_json, 'w') as f:
        json.dump(valid_records, f, indent=4)

    # Check for uniqueness where required
    for key in UNIQUE_FIELDS:
        try:
            dup = metatable.duplicated(key, keep=False)
        except KeyError:
            pass
        if sum(dup) != 0:
            isolate_ids = metatable.loc[dup]['isolate_id'].to_list()
            values = metatable.loc[dup][key].to_list()
            for id, val in zip(isolate_ids, values):
                validation_status[id]["STATUS"] = "FAIL"
                validation_status[id]["MESSAGES"].append(
                    f"Duplicated value in field '{key}': {val}"
                )

    # Export JSON
    with open(json_path, 'w') as f:
        json.dump(validation_status, f, indent=4)

    # Create DF and export to tsv
    dict_to_df = {
        k: [v['STATUS'], ';'.join(v['MESSAGES'])]
        for k, v in validation_status.items()
    }
    pd.DataFrame.from_dict(
        dict_to_df,
        orient='index',
        columns=['STATUS', 'MESSAGES']
    ).reset_index(
        names='isolate_id'
    ).to_csv(
        tsv_path,
        sep='\t',
        header=True,
        index=False
    )


if __name__ == '__main__':
    main(
        snakemake.params['metadata'],
        snakemake.output['json'],
        snakemake.output['tsv'],
        snakemake.output['metadata_json']
    )
