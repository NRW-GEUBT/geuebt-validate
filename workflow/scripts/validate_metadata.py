#!/usr/bin/env python3
# -*- coding: utf-8 -*-


import sys


# if not calling for snakemake rule 
try:
    sys.stderr = open(snakemake.log[0], "w")
except NameError: 
    pass


import json
import jsonschema
import numpy as np
import pandas as pd


VALIDATOR_VERSION = jsonschema.Draft202012Validator
FORMAT_CHECKER=jsonschema.FormatChecker(['date'])
UNIQUE_FIELDS = ['isolate_id', 'fasta_name']
# Only specifying robust types (str or object), specific converters are used
# for dtypes that might fail
INPUT_DTYPE = {
    "isolate_id": str,
    "sample_id": str,
    "organism": str,
    "isolate_name_alt": str,
    "isolation_org": str,
    "sequencing_org": str,
    "extraction_method": str,
    "library_method": str,
    "sequencing_instrument": str,
    "bioinformatics_org": str,
    "assembly_method": str,
    # "third_party_flag": str,
    "third_party_owner": str,
    "sample_type": str,
    "fasta_name": str,
    "fasta_md5": str,
    "collection_date": str,
    "collection_municipality": str,
    "collection_country": str,
    "collection_cause": str,
    "collected_by": str,
    "manufacturer": str,
    "designation": str,
    "manufacturer_type": str,
    "sample_description": str,
    "lot_number": str,
    # "sequencing_depth": np.float64,
    # "ref_coverage": np.float64,
    # "q30": np.float64
}


def boolean_converter(x):
    """
    handle boolean datatype in pd.read_csv
    
    Specifically handle cases where missing values or invalid values 
    result in the whole column being unable to convert to boolean dtype
    """
    if x in ['True', 'true', 'TRUE', True]:
        return 'true'
    elif x in ['False', 'false', 'FALSE', False]:
        return 'false'
    else:
        # If invalid value, let the JSON validation elegantly fail
        return x


def float_converter(x):
    """
    handle float datatype in pd.read_csv
    
    Specifically handle cases where invalid values 
    result in the whole column being unable to convert to float dtype
    """
    try:
        return np.float64(x)
    except ValueError:
        # If invalid value, let the JSON validation elegantly fail
        return pd.NA


def validate_record(record, validator):
    """
    Validate a single JSON record against a valid schema
    
    Each entry of the record is checked against the schema. If errors are 
    encountered, all o them are formattred in a user-friendly way.
    The output is a tuple whose first element is the check result and the 
    second element is the list of formatted error messages.
    
    Parameters
    ----------
    record : dict
        A dictionnary of key-values generated by json.load.
    validator: dict
        A valid instance of jsonschema.protocols.Validator.
    
    Returns
    -------
    tuple:
        a tuple (STATUS, Errors) where status is either of PASS or FAIL and 
        Errors us a list of all validation errors encountered.
    """
    # Get all validations errors and format
    errors = list(validator.iter_errors(record))
    messages = []
    if len(errors) == 0:
        status = "PASS"
    else:
        status = "FAIL"
        for error in errors:
            messages.append(
                f"Invalid value in field {error.schema_path[-2]}. "
                f"Expected {error.schema_path[-1]}: {error.validator_value}, "
                f"got {error.instance}"
            )
    return status, messages


def main(schema, metadata, json_path, tsv_path):
    # load schema as a json object and validate
    with open(schema, 'r') as f:
        json_schema = json.load(f)
    VALIDATOR_VERSION.check_schema(json_schema)
    validator = VALIDATOR_VERSION(
        json_schema, 
        format_checker=FORMAT_CHECKER
    )

    # load metadata as dataframe and create json iterator
    metatable = pd.read_csv(
        metadata,
        sep='\t',
        header=0,
        index_col=False,
        dtype=INPUT_DTYPE,
        converters={
            "third_party_flag": boolean_converter,
            "sequencing_depth": float_converter,
            "ref_coverage": float_converter,
            "q30": float_converter
        }
    )
    records = json.loads(
        metatable.to_json(orient='records')
    )

    # Validate each record and register validation errors
    validation_status = {}
    for record in records:
        status, errors = validate_record(record, validator)
        validation_status.update(
            {
                record["isolate_id"]: {
                    "STATUS": status, 
                    "MESSAGES": errors
                }
            }
        )

    # Check for uniqueness where required
    for key in UNIQUE_FIELDS:
        dup = metatable.duplicated(key, keep=False)
        if sum(dup) != 0:
            isolate_ids = metatable.loc[dup]['isolate_id'].to_list()
            values =  metatable.loc[dup][key].to_list()
            for id, val in zip(isolate_ids, values):
                validation_status[id]["STATUS"] = "FAIL"
                validation_status[id]["MESSAGES"].append(
                    f"Duplicated value in field {key}: {val}"
                )

    # Export JSON
    with open(json_path, 'w') as f:
        json.dump(validation_status, f, indent=4)

    # Create DF and export to tsv
    dict_to_df = {
        k: [v['STATUS'], ';'.join(v['MESSAGES'])]
        for k, v in validation_status.items()
    }
    pd.DataFrame.from_dict(
        dict_to_df, 
        orient='index', 
        columns=['STATUS', 'MESSAGES']
    ).reset_index(
        names='isolate_id'
    ).to_csv(
        tsv_path,
        sep='\t',
        header=True,
        index=False
    )


if __name__ == '__main__':
    main(
        snakemake.input['schema'],
        snakemake.input['metadata'],
        snakemake.output['json'],
        snakemake.output['tsv']
    )
